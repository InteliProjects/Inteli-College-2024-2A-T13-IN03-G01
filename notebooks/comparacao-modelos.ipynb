{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparação de Modelos Preditivos\n",
    "\n",
    "&emsp;A comparação de modelos busca identificar aquele com melhor desempenho para um problema específico, utilizando métricas que quantificam sua eficácia preditiva. Alguns modelos se sobressaem em determinadas métricas, enquanto outros se destacam em diferentes aspectos. Essa análise permite compreender os _trade-offs_ entre as métricas e selecionar o modelo que oferece o melhor equilíbrio entre precisão, viabilidade e facilidade de interpretação, facilitando a escolha mais adequada para o problema em questão.\n",
    "\n",
    "## Comparação de Modelos Não Supervisionados\n",
    "\n",
    "Os modelos preditivos não supervisionados identificam padrões em dados não rotulados, ao contrário dos supervisionados, que dependem de exemplos classificados. A comparação entre abordagens, como _K-means_ e _DBScan_, é essencial, pois cada algoritmo reage de maneira distinta à variabilidade, densidade dos dados e ruídos. Essa análise ajuda a escolher o modelo mais adequado, considerando a sensibilidade a outliers e a capacidade de lidar com dados complexos. Além disso, a avaliação contínua dos modelos permite otimizações, aprimorando a identificação de padrões e tendências em diferentes cenários.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ressalva importante:** esse notebook requer muito processamento, por isso, algumas células podem estar desativadas e pode ocasionar em erros de execução. Por favor, verifique se o ambiente de execução está configurado corretamente e se o hardware é suficiente para rodar o código. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instalação das bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mais informações de como rodar, veja o arquivo `orientacao.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib seaborn plotly scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _Importings_ das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # pandas\n",
    "import numpy as np # numpy\n",
    "import matplotlib.pyplot as plt  # matplotlib\n",
    "import seaborn as sns  # seaborn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Leitura do _DataFrame_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_updated.csv')\n",
    "features = [\"faixa-etaria_encoded_standardized\",\"valor-pago-sinistro_standardized\", \"descricao_servico_sinistro_encoded_standardized\", \"doenca_relacionadas_encoded_standardized\", \"sexo_encoded\", \"tipo_servico_encoded_standardized\"]\n",
    "X = df[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;Como se trata de um modelo não supervisionado, métricas como acurácia, precisão, _recall_ e _F1-score_ não são aplicáveis para sua avaliação. Em vez disso, utilizamos três métricas: _Silhouette Score_, _Calinski-Harabasz Score_ e _Davies-Bouldin Score_. O _Silhouette Score_ mede a similaridade de um objeto com seu próprio cluster em comparação com outros clusters, variando de -1 a 1, sendo que valores próximos de 1 indicam melhor desempenho. O _Calinski-Harabasz Score_ calcula a razão entre a dispersão intra-cluster e a dispersão inter-cluster, com valores maiores indicando melhores resultados. Por fim, o _Davies-Bouldin Score_ avalia a média das similaridades entre cada _cluster_ e seu _cluster_ mais similar, onde valores menores são mais favoráveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusterização: K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;O primeiro modelo analisado foi o *_K-Means_*, um dos algoritmos de clusterização mais difundidos. O algoritmo se baseia na idéia de que cada _cluster_ possui um centroide, que representa o ponto médio de um grupo de dados. O algoritmo funciona iterativamente, ajustando os pontos centrais conforme novos dados são avaliados. Cada dado é alocado ao _cluster_ cujo centroide está mais próximo, utilizando uma medida de distância, como a euclidiana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Informações iniciais do _Dataframe_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;O _Elbow Method_ é uma forma de encontrar a quantidade ideal de clusters (K ideal) para se utilizar no algoritimo K-Means, uma vez que o algoritimo precisa que a quantidade de _clusters_ que serão designados seja definido previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(df[features])\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Número de clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp; O código a sequir realiza a criação do modelo _K-Means_ para agrupar os dados numéricos em 4 clusters, sendo que a quantidade de _clusters_ foi definida anteriormente a partir do _Elbow Method_, uma técnica que auxilia na escolha do valor ideal de K. Após o treinamento do modelo, cada ponto é classificado em um dos 4 _clusters_, e os centros de cada _cluster_ são exibidos. Além disso, a quantidade de elementos pertencentes a cada _cluster_ é calculada e apresentada no final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0, max_iter=100000)\n",
    "numeric_cols = df[features]\n",
    "\n",
    "kmeans.fit(X)\n",
    "\n",
    "df['cluster'] = kmeans.predict(X)\n",
    "print(kmeans.cluster_centers_)\n",
    "df[\"cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;Para reduzir a dimensionalidade dos dados, foi utilizado o algoritmo PCA, que é uma técnica de redução dimensional projetada para simplificar grandes conjuntos de dados enquanto preserva o máximo de variabilidade possível. O PCA transforma os dados originais em novas variáveis, denominadas componentes principais (```PCA1``` e ```PCA2```), que são combinações lineares das variáveis originais.\n",
    "\n",
    "&emsp;Dessa forma, os dados são reduzidos a 2 componentes principais para facilitar a visualização dos clusters formados pelo _K-Means_. Esses componentes são incorporados ao DataFrame e utilizados para criar um gráfico de dispersão, onde os pontos são coloridos de acordo com o _cluster_ a que pertencem. Os centróides dos _clusters_ também são plotados no gráfico, mostrando os centros dos grupos, como pode ser observado na visualização.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar PCA para reduzir os dados a 2 componentes principais\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Adicionar os componentes principais ao DataFrame para visualização\n",
    "df['PCA1'] = X_pca[:, 0]\n",
    "df['PCA2'] = X_pca[:, 1]\n",
    "\n",
    "# Plotar os resultados em um gráfico de dispersão\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.scatterplot(x='PCA1', y='PCA2', hue='cluster', data=df, palette='viridis', style='cluster', markers=['o', 's', '^'])\n",
    "\n",
    "# Plotar os centróides dos clusters em cima do gráfico PCA\n",
    "centroids_pca = pca.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], s=300, c='red', marker='X', label='Centroides')\n",
    "\n",
    "plt.title('Visualização dos Clusters com PCA e KMeans')\n",
    "plt.xlabel('Componente Principal 1')\n",
    "plt.ylabel('Componente Principal 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp; Este _script_ verifica se algumas colunas estão presentes no DataFrame ```df```. Se estiverem, ele gera um gráfico de dispersão (_scatter plot_) onde os dados são coloridos de acordo com os _clusters_ definidos pelo K-Means. O gráfico exibe a relação entre a quantidade e o valor pago do sinistro, facilitando a visualização dos agrupamentos nos dados. Caso as colunas não estejam presentes, é exibida uma mensagem informando que as colunas necessárias estão ausentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Verificar se as colunas existem no DataFrame\n",
    "if 'quantidade_standardized' in df.columns and 'valor-pago-sinistro_standardized' in df.columns and 'cluster' in df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Scatter plot com coloração pelos clusters\n",
    "    sns.scatterplot(\n",
    "        x=\"quantidade_standardized\",\n",
    "        y=\"valor-pago-sinistro_standardized\",\n",
    "        hue='cluster',\n",
    "        data=df,\n",
    "        palette='viridis',\n",
    "        s=100\n",
    "    )\n",
    "\n",
    "    plt.title('Clusters de KMeans')\n",
    "    plt.xlabel('Quantidade')\n",
    "    plt.ylabel('Valor Pago Sinistro')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"As colunas necessárias não estão presentes no DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;Para uma visualização mais clara, foi criado um gráfico 3D que mostra os _clusters_ formados pelo _K-Means_. A visualização 3D dos _clusters_ é gerada utilizando o _Plotly Express_, começando com a criação de um _DataFrame_ chamado ```df_clusters```, que contém as colunas padronizadas e a coluna de clusters. Em seguida, é gerado um gráfico de dispersão com três dimensões, onde as coordenadas são baseadas em três variáveis padronizadas: ```faixa-etaria_encoded_standardized```, ```valor-pago-sinistro_standardized``` e ```descricao_servico_sinistro_encoded_standardized```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters = pd.DataFrame({\n",
    "    'faixa-etaria_encoded_standardized': X['faixa-etaria_encoded_standardized'],\n",
    "    'valor-pago-sinistro_standardized': X['valor-pago-sinistro_standardized'],\n",
    "    'descricao_servico_sinistro_encoded_standardized': X['descricao_servico_sinistro_encoded_standardized'],\n",
    "    'cluster': df['cluster']\n",
    "})\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df_clusters,\n",
    "    x='faixa-etaria_encoded_standardized',\n",
    "    y='valor-pago-sinistro_standardized',\n",
    "    z='descricao_servico_sinistro_encoded_standardized',\n",
    "    color='cluster',\n",
    "    title='Clusters de KMeans em 3D',\n",
    "    opacity=1.0,\n",
    "    size_max=10\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp; Este código ajusta o _StandardScaler_ aos dados originais e, em seguida, utiliza-o para reverter a normalização dos centróides dos _clusters_, retornando-os ao espaço original. Os centróides são então convertidos em um DataFrame para facilitar a interpretação. O código exibe os centróides de cada _cluster_ no espaço original e mostra a contagem de elementos em cada _cluster_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the StandardScaler before using it to transform the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "# Obtain the centroids in the original space (before normalization)\n",
    "centroids_original_space = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "\n",
    "# Convert the centroids to a DataFrame for easier interpretation\n",
    "centroids_df = pd.DataFrame(centroids_original_space, columns=numeric_cols.columns)\n",
    "\n",
    "# Display the centroids of each cluster\n",
    "print(\"Centroids of Clusters in the Original Space:\")\n",
    "print(centroids_df)\n",
    "\n",
    "# View the count of elements in each cluster\n",
    "print(df[\"cluster\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp; Criação de uma matriz de dispersão (_scatter matrix_) usando _Plotly Express_ para visualizar a relação entre várias variáveis dos _clusters_. Cada ponto é colorido de acordo com o _cluster_ a que pertence, facilitando a comparação das variáveis dentro de cada _cluster_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_matrix = px.scatter_matrix(\n",
    "    df_clusters,\n",
    "    dimensions=['descricao_servico_sinistro_encoded_standardized', 'faixa-etaria_encoded_standardized', 'valor-pago-sinistro_standardized'],\n",
    "    color='cluster',\n",
    "    title='Scatter Matrix entre Variáveis por Cluster',\n",
    "    labels={   # Definir rótulos claros para as variáveis\n",
    "        'descricao_servico_sinistro_encoded_standardized': 'Código Serviço',\n",
    "        'faixa-etaria_encoded_standardized': 'Faixa Etária',\n",
    "        'valor-pago-sinistro_standardized': 'Valor Pago'\n",
    "    },\n",
    "    height=900,  # Aumentar a altura para reduzir sobreposição\n",
    "    width=900    # Aumentar a largura\n",
    ")\n",
    "\n",
    "# Ajustando o layout para evitar sobreposição das labels\n",
    "fig_matrix.update_layout(\n",
    "    title={'x':0.5},  # Centraliza o título\n",
    "    font=dict(size=10),  # Reduz o tamanho da fonte\n",
    "    margin=dict(l=50, r=50, b=100, t=100),  # Ajusta as margens\n",
    "    autosize=False\n",
    ")\n",
    "\n",
    "# Mostrar o gráfico\n",
    "fig_matrix.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cálculo das métricas de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar o método de Silhouette, Davis-Bouldin e Calinski-Harabasz\n",
    "silhouette = silhouette_score(X, df['cluster'])\n",
    "print(f\"Silhouette Score: {silhouette}\")\n",
    "davies_bouldin = davies_bouldin_score(X, df['cluster'])\n",
    "print(f\"Davies-Bouldin Score: {davies_bouldin}\")\n",
    "calinski_harabasz = calinski_harabasz_score(X, df['cluster'])\n",
    "print(f\"Calinski-Harabasz Score: {calinski_harabasz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cálculo da melhor combinação de hiperparâmetros para otimização do modelo usando _Grid Search_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_scorer(estimator, X):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    return silhouette_score(X, labels)\n",
    "\n",
    "# Função para calcular Calinski-Harabasz Score\n",
    "def calinski_harabasz_scorer(estimator, X):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    return calinski_harabasz_score(X, labels)\n",
    "\n",
    "# Função para calcular Davies-Bouldin Score (menor é melhor)\n",
    "def davies_bouldin_scorer(estimator, X):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    return davies_bouldin_score(X, labels)\n",
    "\n",
    "\n",
    "# Parâmetros para o GridSearchCV\n",
    "param_grid = {\n",
    "    'n_clusters': [4, 5, 6, 7, 8, 9, 10],  # Ampliar a variação de clusters\n",
    "    'init': ['k-means++', 'random'],      # Testar inicializações diferentes\n",
    "    'max_iter': [300, 500, 1000],         # Variar número de iterações\n",
    "    'n_init': [10, 20, 30, 40],           # Testar diferentes inicializações\n",
    "    'random_state': [0]                   # Manter a aleatoriedade fixa\n",
    "}\n",
    "\n",
    "\n",
    "# Instancia o modelo KMeans\n",
    "kmeans = KMeans()\n",
    "\n",
    "# Configurar o GridSearchCV para o Silhouette Score\n",
    "grid_search_silhouette = GridSearchCV(estimator=kmeans, param_grid=param_grid,\n",
    "                                      scoring=silhouette_scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "# Ajustar o modelo com os dados\n",
    "grid_search_silhouette.fit(df[features])\n",
    "\n",
    "# Melhor modelo e pontuação para Silhouette Score\n",
    "print(\"Melhores Parâmetros (Silhouette):\", grid_search_silhouette.best_params_)\n",
    "print(\"Melhor Silhouette Score:\", grid_search_silhouette.best_score_)\n",
    "\n",
    "# Configurar o GridSearchCV para o Calinski-Harabasz Score\n",
    "grid_search_calinski = GridSearchCV(estimator=kmeans, param_grid=param_grid,\n",
    "                                    scoring=calinski_harabasz_scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "# Ajustar o modelo com os dados\n",
    "grid_search_calinski.fit(df[features])\n",
    "\n",
    "# Melhor modelo e pontuação para Calinski-Harabasz Score\n",
    "print(\"Melhores Parâmetros (Calinski-Harabasz):\", grid_search_calinski.best_params_)\n",
    "print(\"Melhor Calinski-Harabasz Score:\", grid_search_calinski.best_score_)\n",
    "\n",
    "# Configurar o GridSearchCV para o Davies-Bouldin Score\n",
    "grid_search_davies = GridSearchCV(estimator=kmeans, param_grid=param_grid,\n",
    "                                  scoring=davies_bouldin_scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "# Ajustar o modelo com os dados\n",
    "grid_search_davies.fit(df[features])\n",
    "\n",
    "# Melhor modelo e pontuação para Davies-Bouldin Score\n",
    "print(\"Melhores Parâmetros (Davies-Bouldin):\", grid_search_davies.best_params_)\n",
    "print(\"Melhor Davies-Bouldin Score:\", -grid_search_davies.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;O _Agglomerative Clustering_ é um algoritmo de clusterização hierárquica que agrupa os dados em _clusters_ de forma iterativa. Ele começa com cada ponto de dados como um _cluster_ separado e, em seguida, combina os _clusters_ mais próximos até que todos os pontos de dados estejam em um único _cluster_. O algoritmo é baseado em uma matriz de distâncias entre os pontos de dados, que é atualizada a cada iteração. O _Agglomerative Clustering_ é um algoritmo eficaz para identificar agrupamentos em dados não rotulados e pode ser usado para explorar a estrutura subjacente dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp; Devido a demanda de muito poder computacional gerada pelo algoritmo, foi criado uma simplificação fracionada dos dados (```dfSample```) e aplicada uma técnica de redução de dimensionalidade (PCA) para a execução do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample = df[features].sample(frac=0.5)\n",
    "X = dfSample.values\n",
    "\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Printa o tamanho dos dados originais\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Criação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esse demora pra carregar, portanto não deve ser rodado. \n",
    "\n",
    "print(\"Agglomerative Clustering\")\n",
    "clustering = AgglomerativeClustering(n_clusters=4, linkage=\"ward\").fit_predict(X_pca)\n",
    "print(\"Agglomerative Clustering done\")\n",
    "\n",
    "\n",
    "# Adicionando os clusters ao dataframe\n",
    "dfSample['cluster'] = clustering\n",
    "dfSample.to_csv('./data_updated_clusters_agg_50.csv', index=False) # Salvar processamento\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    dfSample,\n",
    "    x='faixa-etaria_encoded_standardized',\n",
    "    y='valor-pago-sinistro_standardized',\n",
    "    z='quantidade_standardized',\n",
    "    color='cluster',  # Colorir com base nos clusters\n",
    "    title='Clusters de Agglomerative Clustering em 3D',\n",
    "    opacity=1.0,\n",
    "    size_max=10\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualização gráfica em 3d do modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_updated_clusters_agg_50.csv')\n",
    "# print df shape\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df,\n",
    "    x='faixa-etaria_encoded_standardized',\n",
    "    y='valor-pago-sinistro_standardized',\n",
    "    z='quantidade_standardized',\n",
    "    color='cluster',  # Colorir com base nos clusters\n",
    "    title='Clusters de Agglomerative Clustering em 3D',\n",
    "    opacity=1.0,\n",
    "    size_max=10\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cálculo das métricas de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar o método de Silhouette, Davis-Bouldin e Calinski-Harabasz\n",
    "silhouette = silhouette_score(X, df['cluster'])\n",
    "print(f\"Silhouette Score: {silhouette}\")\n",
    "davies_bouldin = davies_bouldin_score(X, df['cluster'])\n",
    "print(f\"Davies-Bouldin Score: {davies_bouldin}\")\n",
    "calinski_harabasz = calinski_harabasz_score(X, df['cluster'])\n",
    "print(f\"Calinski-Harabasz Score: {calinski_harabasz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cálculo da melhor combinação de hiperparâmetros para otimização do modelo usando _Random Search_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir função customizada para o silhouette score\n",
    "def custom_silhouette_scorer(estimator, X):\n",
    "    labels = estimator.fit_predict(X)  # Realizar a predição\n",
    "    score = silhouette_score(X, labels, metric='euclidean')  # Calcular o silhouette score\n",
    "    return score\n",
    "\n",
    "# Parâmetros para busca\n",
    "param_distributions = {\n",
    "    'n_clusters': [4, 5, 6, 7, 8, 9, 10],  # Número de clusters\n",
    "    'linkage': ['complete', 'average', 'single'],\n",
    "    'metric': ['euclidean', 'manhattan', 'cosine']  # O parâmetro correto é \"metric\"\n",
    "}\n",
    "\n",
    "# Inicializar o RandomizedSearchCV com AgglomerativeClustering\n",
    "agg_cluster = AgglomerativeClustering()\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=agg_cluster,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=3,  # Número de combinações aleatórias a tentar\n",
    "    scoring=custom_silhouette_scorer,  # Função customizada para o silhouette score\n",
    "    cv=3,  # Validação cruzada (não muito relevante para clustering)\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Fazer o ajuste com os dados (X_pca é o conjunto de dados PCA transformado)\n",
    "random_search.fit(X_pca)\n",
    "\n",
    "# Resultados\n",
    "print(\"Melhores parâmetros: \", random_search.best_params_)\n",
    "print(\"Melhor score (silhouette): \", random_search.best_score_)\n",
    "print(\"Davies-Bouldin Score: \", davies_bouldin_score(X, random_search.best_estimator_.labels_))\n",
    "print(\"Calinski-Harabasz Score: \", calinski_harabasz_score(X, random_search.best_estimator_.labels_))\n",
    "print(\"Melhor modelo: \", random_search.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;O *_DBSCAN_* (_Density-Based Spatial Clustering of Applications with Noise_) é um algoritmo de agrupamento que se baseia na densidade dos pontos em um espaço, ao contrário de métodos como o *_K-Means_*, que utilizam distâncias para formar _clusters_. Ele é eficaz na detecção de clusters de forma arbitrária e na identificação de ruídos nos dados, classificando pontos isolados como _outliers_. Essa abordagem permite que o _*DBSCAN_* crie grupos compactos e bem definidos, sendo ideal para conjuntos de dados com formas complexas e distribuições desiguais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Funções utilizadas para a criação do modelo DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan(X, eps, min_samples):\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
    "    labels = db.labels_\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "    print('Estimated number of clusters: %d' % n_clusters_)\n",
    "    print('Estimated number of noise points: %d' % n_noise_)\n",
    "    return db\n",
    "\n",
    "def metricas_de_modelo(X, labels):\n",
    "    silhouette = metrics.silhouette_score(X, labels)\n",
    "    print(\"Silhouette Coefficient: %0.3f\" %silhouette)\n",
    "    \n",
    "    calisnki = metrics.calinski_harabasz_score(X, labels)\n",
    "    print(\"Calinski Harabasz Score: %0.3f\" %calisnki)\n",
    "    \n",
    "    davies = metrics.davies_bouldin_score(X, labels)\n",
    "    print(\"Davies Bouldin Score: %0.3f\" %davies)\n",
    "\n",
    "def silhouette_scorer(estimator, X):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    return silhouette_score(X, labels)\n",
    "\n",
    "# Função para calcular Calinski-Harabasz Score\n",
    "def calinski_harabasz_scorer(estimator, X):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    return calinski_harabasz_score(X, labels)\n",
    "\n",
    "# Função para calcular Davies-Bouldin Score (menor é melhor)\n",
    "def davies_bouldin_scorer(estimator, X):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    return davies_bouldin_score(X, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cálculo da melhor combinação de hiperparâmetros para otimização do modelo usando _Grid Search_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'eps': [0.001, 0.01, 0.1, 1, 5, 10],\n",
    "    'min_samples': [1, 2, 3, 5, 10, 20]\n",
    "}\n",
    "\n",
    "dbscan = DBSCAN()\n",
    "\n",
    "# Configurar o GridSearchCV para o Silhouette Score\n",
    "grid_search_silhouette = GridSearchCV(estimator=dbscan, param_grid=param_grid,\n",
    "                                      scoring=silhouette_scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "# Ajustar o modelo com os dados\n",
    "grid_search_silhouette.fit(X)\n",
    "\n",
    "# Melhor modelo e pontuação para Silhouette Score\n",
    "print(\"Melhores Parâmetros (Silhouette):\", grid_search_silhouette.best_params_)\n",
    "print(\"Melhor Silhouette Score:\", grid_search_silhouette.best_score_)\n",
    "\n",
    "# Configurar o GridSearchCV para o Calinski-Harabasz Score\n",
    "grid_search_calinski = GridSearchCV(estimator=dbscan, param_grid=param_grid,\n",
    "                                    scoring=calinski_harabasz_scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "# Ajustar o modelo com os dados\n",
    "grid_search_calinski.fit(X)\n",
    "\n",
    "# Melhor modelo e pontuação para Calinski-Harabasz Score\n",
    "print(\"Melhores Parâmetros (Calinski-Harabasz):\", grid_search_calinski.best_params_)\n",
    "print(\"Melhor Calinski-Harabasz Score:\", grid_search_calinski.best_score_)\n",
    "\n",
    "# Configurar o GridSearchCV para o Davies-Bouldin Score\n",
    "grid_search_davies = GridSearchCV(estimator=dbscan, param_grid=param_grid,\n",
    "                                  scoring=davies_bouldin_scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "# Ajustar o modelo com os dados\n",
    "grid_search_davies.fit(X)\n",
    "\n",
    "# Melhor modelo e pontuação para Davies-Bouldin Score\n",
    "print(\"Melhores Parâmetros (Davies-Bouldin):\", grid_search_davies.best_params_)\n",
    "print(\"Melhor Davies-Bouldin Score:\", grid_search_davies.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cálculo das métricas de performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbscan(X, eps=1, min_samples=10)\n",
    "labels_dbscan = db.labels_\n",
    "metricas_de_modelo(X, labels_dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Análise das métricas do DBSCAN sem _outliers_ (ruído)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = labels_dbscan != -1  \n",
    "X_no_noise = X[mask]\n",
    "labels_no_noise = labels_dbscan[mask]\n",
    "\n",
    "metricas_de_modelo(X_no_noise, labels_no_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plotagem do gráfico usando PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplicado o PCA para reduzir a dimensionalidade e possibilidade a execução do modelo.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels_dbscan, cmap='viridis', s=50)\n",
    "plt.title('Spectral Clustering com PCA - 2 clusters')\n",
    "plt.xlabel('Componente Principal 1')\n",
    "plt.ylabel('Componente Principal 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;O _Gaussian Mixture Model_ (GMM) é um modelo probabilístico utilizado para clusterização, que parte do princípio de que os dados foram gerados a partir de uma combinação de várias distribuições Gaussianas. Cada distribuição Gaussiana representa um _cluster_ nos dados, e o GMM busca identificar esses clusters ajustando as distribuições em relação aos dados observados. Ao contrário de algoritmos de clusterização como o *_K-Means_*, que designam cada ponto de dados a um cluster específico, o GMM oferece maior flexibilidade ao atribuir uma probabilidade de pertencimento a cada cluster. Assim, um ponto de dados pode estar associado a múltiplos _clusters_, com diferentes níveis de probabilidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Funções utilizadas para a criação do modelo DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo a função para o Gaussian Mixture Model\n",
    "def gaussian_mixture_model(X, n_components):\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "    labels = gmm.fit_predict(X)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIC - Bayesian Information Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&ensp;A _Bayesian Information Criterion_ (BIC) é uma métrica usada para seleção de modelos preditivos e aprendizado de máquina. A BIC ajuda a comparar diferentes modelos com diferentes graus de complexidade, penalizando aqueles que possuem muitos parâmetros, para evitar _overfitting_ (ajuste excessivo aos dados). O melhor modelo, seria aquele que teoricamente possísse um valor menor de BIC, mas ao plotar em um gráfico os valores com seus respectivos parâmetros, podemos visualizar qual o que atinge um bom valor de BIC, mas com o menor valor de parâmetro ```n_components```, sendo um ponto bem onde os _clusters_ tem uma boa qualidade, mas com menor complexidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bic(X):\n",
    "    n_components_range = range(1, 14)\n",
    "    bic_scores = []\n",
    "\n",
    "    for n in n_components_range:\n",
    "        gmm = GaussianMixture(n_components=n, random_state=42)\n",
    "        gmm.fit(X)\n",
    "        \n",
    "        bic_scores.append(gmm.bic(X))\n",
    "\n",
    "    # Plot AIC and BIC to find the optimal number of components\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(n_components_range, bic_scores, label='BIC')\n",
    "    plt.xlabel('Número de Componentes')\n",
    "    plt.ylabel('BIC')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Aplicando o método BIC na variável ```X```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Aplicando o valor de ```n_components```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_gmm = gaussian_mixture_model(X, n_components=2)\n",
    "metricas_de_modelo(X, labels_gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Aplicação do PCA para redução da dimensionalidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels_gmm, cmap='viridis', s=50)\n",
    "plt.title('Spectral Clustering com PCA - 2 clusters')\n",
    "plt.xlabel('Componente Principal 1')\n",
    "plt.ylabel('Componente Principal 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&ensp;Para escolher o melhor modelo de clustering, analisamos três métricas principais: Calinski-Harabasz Index, Silhouette Score e Davies-Bouldin Index.\n",
    "\n",
    "&ensp;O Calinski-Harabasz Index, que mede a qualidade dos clusters com base nas distâncias intra e inter-cluster, não foi utilizado para comparar modelos, pois é sensível ao número de clusters e pode ser influenciado por outliers, não sendo uma boa métrica de comparação entre diferentes modelos.\n",
    "\n",
    "&ensp;O Silhouette Score, que mede a coesão dos clusters, mostrou que o DBSCAN (0.830) e o Agglomerative Clustering (0.721) tiveram o melhor desempenho, formando clusters bem definidos. Já KMeans(0.580) e GMM tiveram valores menores, indicando menos coesão.\n",
    "\n",
    "&ensp;O Davies-Bouldin Index, que avalia a separação entre clusters, mostrou que o DBSCAN teve o melhor desempenho (0.191), com clusters bem separados e baixa sobreposição. O Agglomerative Clustering também se saiu bem (0.455), enquanto GMM (3.045) e KMeans (0.202) apresentaram maior sobreposição.\n",
    "\n",
    "&ensp;Portanto, o DBSCAN foi o modelo com melhor desempenho geral, no entanto, analisando a coerência dos clusters, o KMeans foi o que mais se destacou e portanto é o modelo mais indicado para o problema em questão."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
