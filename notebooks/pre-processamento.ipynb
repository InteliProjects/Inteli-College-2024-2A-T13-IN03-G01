{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise exploratória\n",
    "\n",
    "&ensp;A análise exploratória dos dados (EDA) investiga conjuntos de dados e pretende resumir suas principais características, empregando métodos de visualização de dados. Além disso, a EDA também proporciona uma melhor compreensão das variáveis do conjunto de dados e das relações entre elas. A etapa de exploração precede o modelo preditivo, pois permite identificar erros óbvios, entender melhor os padrões nos dados, detectar valores discrepantes ou eventos anômalos e encontrar relações interessantes entre as variáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mais informações de como rodar, veja o arquivo `orientacao.md`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importação das bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # pandas\n",
    "import numpy as np # numpy\n",
    "import matplotlib.pyplot as plt  # matplotlib\n",
    "import seaborn as sns  # seaborn\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Leitura e visualização do DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./data.xlsx') # leitura do arquivo\n",
    "df.head() # visualização do Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estatísticas descritivas básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df[[\"Valor Pago Sinistro\", \"Quantidade\", \"No Ano Mes\"]] # seleção de colunas numéricas\n",
    "basic_stats = numeric_cols.describe()\n",
    "basic_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outras estatísticas descritivas:\n",
    "* Variância\n",
    "* Desvio padrão\n",
    "* Coeficiente de Variação\n",
    "* Assimetria\n",
    "* Curtose\n",
    "* Correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = numeric_cols.var()\n",
    "standart_deviation = numeric_cols.std()\n",
    "coef_variation = standart_deviation / numeric_cols.mean()\n",
    "skew = numeric_cols.skew()\n",
    "kurtosis = numeric_cols.kurtosis()\n",
    "correlation = numeric_cols.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento de dados\n",
    "\n",
    "&ensp;O pré-processamento de dados envolve a preparação dos dados brutos para que possam ser usados de forma eficaz por algoritmos de modelagem. Essa etapa garante que os dados estejam em um formato limpo, consistente e pronto para análise ou construção de modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&ensp;Em primeira análise, foi descoberto a presença de dois valores com a mesma atribuição qualitativa, \"UNIPAR INDUPA DO BRASIL S.A\" e \"UNIPAR INDUPA DO BRASIL S.A.\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Nome Empresa Sinistro\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Nome Empresa Sinistro\"] = df[\"Nome Empresa Sinistro\"].replace(\"UNIPAR INDUPA DO BRASIL S.A.\", \"UNIPAR INDUPA DO BRASIL S.A\")\n",
    "df[\"Nome Empresa Sinistro\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&ensp;Além disso, foi verificada a presença de variáveis com valor nulo (Missing Values), onde o resultado foi positivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&ensp;A coluna \"No Ano Mês\" foi normalizada para adequar strings de data para objetos datetime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"no-ano-mes_stardardized\"] = pd.to_datetime(df[\"No Ano Mes\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&ensp;A coluna \"Faixa-Etária Nova Sinistro\", classificada como categórica ordenada, foi codificada por meio do método Label Encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"faixa-etaria_encoded\"] = df[\"Faixa-Etária Nova Sinistro\"].astype('category').cat.codes\n",
    "df[\"faixa-etaria_encoded\"].value_counts()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&ensp;As variáveis \"Valor Pago Sinistro\" e \"Quantidade\" foram padronizadas usando o método _Standart Scaler_, utilizado para padronizar as features de um conjunto de dados, ou seja, para transformá-las de modo que tenham média igual a 0 e desvio padrão igual a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identificação de outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&ensp;A identificação de outliers é uma etapa fundamental, pois esses pontos podem distorcer análises estatísticas e prejudicar o desempenho do modelo. </br>\n",
    "&ensp;Primeiramente, foram identificadas, na base de dados as variáveis numéricas que apresentam um padrão ou tendencia que se relacionar com as outras linhas (variáveis que poderiam ter outliers), que foram as colunas de 'Valor Pago Sinistro' e 'Quantidade', elas se referem ao valor de pagamento efetuado e a quantidade de serviços que aquele pagamento representa, respectivamente.</br>\n",
    "&ensp;Após isso, foi feita uma análise gráfica de cada uma delas para identificar possíveis dados que se destacam ou fogem do padrão dos outros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coluna 'Valor Pago Sinistro'\n",
    "\n",
    "&ensp; Para a visualização gráfica da coluna 'Valor Pago Sinistro, é possível fazer um gráfico de caixas (boxplot), onde os pontos muito distoantes são outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['Valor Pago Sinistro'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&ensp;Após analisar o gráfico, é possivel perceber que no intervalo de 0 até um pouco antes de 100 mil os pontos quase formam uma linha contínua, algo que para de se cumprir quando o valor pago de sinistro ultrapassa os 100 mil reais. Logo, pode-se analisar que essas ocorrências são muito raras e distôam da maioria. Ainda assim, não podem ser considerados _outliers_, pois isso depende da forma com a qual nós os utilizariamos. Em certas situações, como ao calcular a média dos valores de sinsitros, eles podem ser considerados, mas só por aparecerem em situações raras, não podemos negligênciar sua ocorrência e importância na base de dados.<br/>\n",
    "&ensp;Dito isso, segue a comprovação da quantidade dessas ocorrências acima de 100 mil reais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocorrencia = df['Valor Pago Sinistro'] > 100000\n",
    "contagem_de_ocorrencia = ocorrencia.value_counts()\n",
    "print(contagem_de_ocorrencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&ensp;Logo, apenas 16 linhas de cem mil, possuem valores acima dos 100 mil reais de valor pago em sinistro. Agora, vamos mostrar como eles afetam a média, e depois qual o seu impacto no valor total gasto em sinistro, para comprovar nossa tese em relação a esses \"_outliers_\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_antes = df['Valor Pago Sinistro'].mean()\n",
    "print(\"A média sem eles é: \" + str(media_antes))\n",
    "\n",
    "media_depois = df[df['Valor Pago Sinistro'] < 100000]['Valor Pago Sinistro'].mean()\n",
    "print('A média depois é: ' + str(media_depois))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&ensp;Agora, o impacto no valor total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_total = df['Valor Pago Sinistro'].sum()\n",
    "soma_ocorrencia = df[df['Valor Pago Sinistro'] > 100000]['Valor Pago Sinistro'].sum()\n",
    "impacto = (soma_ocorrencia / valor_total) * 100\n",
    "\n",
    "# Impressão dos valores para verificação e do impacto formatado\n",
    "print(f'Valor Total: {valor_total}')\n",
    "print(f'Soma das Ocorrências (valores > 100000): {soma_ocorrencia}')\n",
    "print(f'É um impacto de {impacto:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sendo assim, essa alteração pode desempenhar um papel significativo quando formos treinar o nosso modelo preditivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coluna 'Quantidade'\n",
    "\n",
    "&ensp; Para visualizar melhor os outliers da coluna 'Quantidade', fizemos outro boxplot. Nele, é possível identificar alguns dados que possuem quantidades iguais a zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['Quantidade'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&ensp; Sendo assim, esses dados não se encaixam no número de quantidades reais e possíveis. Portanto, esses dados devem ser alterados. Pensando nisso, calculamos a moda - o valor que mais se repete - dessa coluna, que foi igual a 1, como pode ser observado na operação feita abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moda = df[\"Quantidade\"].mode()\n",
    "moda.index = ['Moda:']\n",
    "print(moda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&ensp; Após calcularmos a moda, substituímos os valores que eram considerados outliers pelo grupo, por 1. Vale ressaltar que essa decisão foi tomada considerando o impacto dos dados. Dessa forma, buscamos minimizar os efeitos desses valores nas análises e no modelo preditivo que estaremos trabalhando na terceira sprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Quantidade'] = df['Quantidade'].replace([0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&ensp; Com as alterações feitas, podemos visualizar o seguinte gráfico como resultado, no qual é possível observar as mudanças. Os outliers foram corrigidos para que não impactem negativamente o projeto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['Quantidade'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inequidade do código do serviço com a descrição\n",
    "\n",
    "&ensp; A coluna 'Código do Serviço' apresenta uma inequidade com a coluna 'Descrição do Serviço', pois, ao analisar a base de dados, percebemos que a contagem de códigos de serviço é menor que a contagem de descrições de serviço. Isso ocorre porque um mesmo código de serviço foi aplicado por engano a diferentes descrições de serviço. Para corrigir essa inequidade, foi feita uma análise para identificar quais códigos de serviço foram aplicados a diferentes descrições de serviço. A partir dessa análise, foi possível identificar os códigos de serviço que foram aplicados a diferentes descrições de serviço e corrigir essa inequidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa o mapeamento com o dicionário correto\n",
    "mapeamento_codigo = {}\n",
    "\n",
    "# Popula o dicionário de mapeamento\n",
    "for index, row in df.iterrows():\n",
    "    if row[\"Codigo Servico Sinistro\"] not in mapeamento_codigo:\n",
    "        mapeamento_codigo[row[\"Codigo Servico Sinistro\"]] = row[\"Descricao Servico Sinistro\"]\n",
    "        mapeamento_codigo[row[\"Descricao Servico Sinistro\"]] = row[\"Codigo Servico Sinistro\"]\n",
    "\n",
    "# Função para verificar inconsistências\n",
    "def verificar_inconsistencia(row):\n",
    "    codigo = row[\"Codigo Servico Sinistro\"]\n",
    "    descricao = row[\"Descricao Servico Sinistro\"]\n",
    "    \n",
    "    # Verifica se o código existe no mapeamento e se a descrição está correta\n",
    "    return codigo in mapeamento_codigo and descricao != mapeamento_codigo[codigo]\n",
    "\n",
    "inconsistencias = df[df.apply(verificar_inconsistencia, axis=1)]\n",
    "# Arrumando inconsistências\n",
    "for index, row in inconsistencias.iterrows():\n",
    "    df.at[index, \"Codigo Servico Sinistro\"] = mapeamento_codigo[row[\"Descricao Servico Sinistro\"]]\n",
    "\n",
    "# Verificando se ainda existem inconsistências\n",
    "inconsistencias = df[df.apply(verificar_inconsistencia, axis=1)]\n",
    "inconsistencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando novos dados de IA generativa\n",
    "&emsp;Nesse processo, utilizamos uma técnica de IA generativa para gerar novos dados que possam ser utilizados para melhorar a performance do modelo preditivo. Os dados enviados foram a descrição do serviço, onde foi pedido que ela alocasse em categorias menos específicas e se o exame era relacionado uma doença em potencial ou não. A partir disso, foi gerado um novo dataset com essas informações, que é utilizado com novas features para o modelo preditivo. Para mais detalhes, consulte a documentação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carrega o JSON de categorias\n",
    "categorias = pd.read_json('./assets/categorias.json')\n",
    "\n",
    "# Itera sobre cada categoria (coluna) do DataFrame de categorias\n",
    "for categoria in categorias.columns:\n",
    "    # Atualiza as colunas \"Doença relacionada\" e \"Tipo de Serviço\" com valores da categoria\n",
    "    df.loc[df[\"Descricao Servico Sinistro\"] == categoria, \"Doença relacionada\"] = categorias[categoria].iloc[1]\n",
    "    df.loc[df[\"Descricao Servico Sinistro\"] == categoria, \"Tipo de Serviço\"] = categorias[categoria].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além disso, outro passo importante foi trocar os textos das colunas adicionadas para números, para que o modelo possa interpretar esses dados. Para isso, foi utilizado o método Label Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo para label encoded para o modelo de machine learning\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelEncoder = LabelEncoder()\n",
    "df[['doenca_relacionadas_encoded', 'tipo_servico_encoded']] = df[['Doença relacionada', 'Tipo de Serviço']].apply(labelEncoder.fit_transform)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padronizando a data para o formato datetime e depois para inteiro, para que o modelo possa interpretar esses dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Dt Data Sinistro\"] = pd.to_datetime(df[\"Dt Data Sinistro\"])\n",
    "df[\"Dt Data Sinistro Padronizada\"] = df[\"Dt Data Sinistro\"].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coletando o dia da semana em que o sinistro foi realizado, assim, podemos verificar a viabilidade de se criar um modelo que leva em consideração o dia da semana em que o sinistro foi realizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Dia da Semana Sinistro\"] = df[\"Dt Data Sinistro\"].dt.day_name(\"portuguese\")\n",
    "df[\"dia_da_semana_sinistro_encoded\"] = df[\"Dia da Semana Sinistro\"].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também foi verificado o impacto de feriados próximos ao dia do sinistro, para verificar se há uma relação entre a ocorrência de sinistros e feriados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "feriados_2023 = pd.read_json('./assets/feriados_2023.json')\n",
    "feriados_2024 = pd.read_json('./assets/feriados_2024.json')\n",
    "\n",
    "# Concatena os feriados de 2023 e 2024\n",
    "feriados = pd.concat([feriados_2023, feriados_2024])\n",
    "\n",
    "feriados[\"date\"] = pd.to_datetime(feriados[\"date\"])\n",
    "\n",
    "\n",
    "# Verifica se a data do sinistro é um feriado em um range de 3 dias para mais ou para menos\n",
    "def verificar_feriado(row):\n",
    "    data = row[\"Dt Data Sinistro\"]\n",
    "    return feriados[\"date\"].between(data - pd.Timedelta(days=3), data + pd.Timedelta(days=3)).any()\n",
    "df[\"Feriado Próximo\"] = df.apply(verificar_feriado, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deixando a coluna 'Elegibilidade' em formato numérico, para que o modelo possa interpretar esses dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"elegibilidade_sinistro_encoded\"] = df[\"Elegibilidade Sinistro\"].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deixando a coluna 'Sexo Sinistro' para numérica para que o modelo possa interpretar esses dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sexo_encoded\"] = df[\"Sexo Sinistro\"].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A coluna 'Descricao Plano Sinistro' foi transformada em numérica para que o modelo possa interpretar esses dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"descricao_plano_sinistro_encoded\"] = df[\"Descricao Plano Sinistro\"].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui foi feita a transformação da variável 'Descricao do Serviço' para numérica, apesar dela possuir uma coluna que transforme ela em númerica, os valores são distoantes e não são úteis para o modelo, então foi feita uma nova transformação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"descricao_servico_sinistro_encoded\"] = df[\"Descricao Servico Sinistro\"].astype('category').cat.codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padronizando as variáveis que tem valores muito altos, para que o modelo possa interpretar esses dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['valor-pago-sinistro_standardized', 'quantidade_standardized', 'descricao_servico_sinistro_encoded_standardized', 'doenca_relacionadas_encoded_standardized', 'tipo_servico_encoded_standardized', 'dia_da_semana_sinistro_encoded_standardized', 'faixa-etaria_encoded_standardized', 'data_sinistro_standardized']] = scaler.fit_transform(df[[\"Valor Pago Sinistro\", \"Quantidade\", \"descricao_servico_sinistro_encoded\", \"doenca_relacionadas_encoded\", \"tipo_servico_encoded\", \"dia_da_semana_sinistro_encoded\", \"faixa-etaria_encoded\", \"Dt Data Sinistro Padronizada\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data_updated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportando os objetos de padronização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Salva o scaler em um arquivo\n",
    "with open('./assets/scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "# Salva o label encoder em um arquivo\n",
    "with open('./assets/label_encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(labelEncoder, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
